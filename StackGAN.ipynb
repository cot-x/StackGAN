{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from comet_ml import Experiment\n",
    "#experiment = Experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Ccpy4OkFMEM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import cv2\n",
    "import time\n",
    "import argparse\n",
    "import itertools\n",
    "import string\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageFile\n",
    "from pickle import load, dump\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "#from torchinfo import summary\n",
    "#from pprint import pprint\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, DataCollatorWithPadding\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert(nn.Module):\n",
    "    #model_name = 'tohoku-nlp/bert-base-japanese-v3'\n",
    "    model_name = 'google-bert/bert-base-cased'\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = AutoModel.from_pretrained(Bert.model_name)\n",
    "    \n",
    "    def forward(self, **x):\n",
    "        x = self.model(**x)['last_hidden_state']\n",
    "        x = x[:, 0, :] # [CLS]\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FReLU(nn.Module):\n",
    "    def __init__(self, n_channel, kernel=3, stride=1, padding=1):\n",
    "        super().__init__()\n",
    "        self.funnel_condition = nn.Conv2d(n_channel, n_channel, kernel_size=kernel,stride=stride, padding=padding, groups=n_channel)\n",
    "        self.normalize = nn.BatchNorm2d(n_channel)\n",
    "\n",
    "    def forward(self, x):\n",
    "        tx = self.normalize(self.funnel_condition(x))\n",
    "        out = torch.max(x, tx)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(in_features),\n",
    "            FReLU(in_features),\n",
    "            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(in_features)\n",
    "        )\n",
    "        self.activate = FReLU(in_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = self.shortcut(x)\n",
    "        return self.activate(self.residual(x) + shortcut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STAGE1_G(nn.Module):\n",
    "    def __init__(self, dim_c_code=768, dim_noise=128, dim_ideal=1024):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dim_noise = dim_noise\n",
    "        \n",
    "        def upBlock(dim_in, dim_out):\n",
    "            block = nn.Sequential(\n",
    "                nn.Upsample(scale_factor=2),\n",
    "                nn.Conv2d(dim_in, dim_out, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(dim_out),\n",
    "                FReLU(dim_out)\n",
    "            )\n",
    "            return block\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(dim_c_code + dim_noise, dim_ideal * 4 * 4),\n",
    "            nn.Mish(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.upsample1 = upBlock(dim_ideal, dim_ideal // 2)       # 8x8\n",
    "        self.upsample2 = upBlock(dim_ideal // 2, dim_ideal // 4)  # 16x16\n",
    "        self.upsample3 = upBlock(dim_ideal // 4, dim_ideal // 8)  # 32x32\n",
    "        self.upsample4 = upBlock(dim_ideal // 8, dim_ideal // 16) # 64x64\n",
    "        \n",
    "        self.toRGB = nn.Sequential(\n",
    "            nn.Conv2d(dim_ideal // 16, 3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, text):\n",
    "        noise = torch.randn(text.size(0), self.dim_noise).to(text.device)\n",
    "        c_code = torch.cat((noise, text), 1)\n",
    "        \n",
    "        estimate = self.encoder(c_code)\n",
    "        \n",
    "        h_code = estimate.view(estimate.size(0), -1, 4, 4)\n",
    "        h_code = self.upsample1(h_code)\n",
    "        h_code = self.upsample2(h_code)\n",
    "        h_code = self.upsample3(h_code)\n",
    "        h_code = self.upsample4(h_code)\n",
    "        fake_img = self.toRGB(h_code)\n",
    "        \n",
    "        _noise = torch.randn(c_code.shape).to(text.device)\n",
    "        true_pdf = self.encoder(_noise).softmax(dim=1)\n",
    "        \n",
    "        # Variational Conditional GAN's Loss\n",
    "        kl_loss = nn.KLDivLoss(reduction='sum')\n",
    "        vc_loss = - kl_loss(estimate.softmax(dim=1), true_pdf)\n",
    "        \n",
    "        return fake_img, vc_loss, noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STAGE1_D(nn.Module):\n",
    "    def __init__(self, dim_c_code=768, dim_ideal=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        def downBlock(dim_in, dim_out):\n",
    "            block = nn.Sequential(\n",
    "                nn.Conv2d(dim_in, dim_out, kernel_size=4, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(dim_out),\n",
    "                FReLU(dim_out)\n",
    "            )\n",
    "            return block\n",
    "        \n",
    "        self.downconv = nn.Sequential(\n",
    "            downBlock(3, dim_ideal),                 # 32x32\n",
    "            downBlock(dim_ideal, dim_ideal * 2),     # 16x16\n",
    "            downBlock(dim_ideal * 2, dim_ideal * 4), # 8x8\n",
    "            downBlock(dim_ideal * 4, dim_ideal * 8)  # 4x4\n",
    "        )\n",
    "        \n",
    "        self.conv_patch = nn.Conv2d(dim_ideal * 8 + dim_c_code, 1, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "    def forward(self, image, text):\n",
    "        cond = self.downconv(image)\n",
    "        \n",
    "        text = text.view(text.size(0), -1, 1, 1)\n",
    "        c_code = text.repeat(1, 1, 4, 4)\n",
    "        \n",
    "        c_code = torch.cat((cond, c_code), 1)\n",
    "        \n",
    "        patch = self.conv_patch(c_code)\n",
    "        return patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STAGE2_G(nn.Module):\n",
    "    def __init__(self, dim_c_code=768, dim_ideal=128, n_residual=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        def upBlock(dim_in, dim_out):\n",
    "            block = nn.Sequential(\n",
    "                nn.Upsample(scale_factor=2),\n",
    "                nn.Conv2d(dim_in, dim_out, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(dim_out),\n",
    "                FReLU(dim_out)\n",
    "            )\n",
    "            return block\n",
    "        \n",
    "        self.downsample = nn.Sequential(\n",
    "            nn.Conv2d(3, dim_ideal, kernel_size=3, stride=1, padding=1),\n",
    "            FReLU(dim_ideal),\n",
    "            nn.Conv2d(dim_ideal, dim_ideal * 2, kernel_size=4, stride=2, padding=1),     # 32x32\n",
    "            nn.BatchNorm2d(dim_ideal * 2),\n",
    "            FReLU(dim_ideal * 2),\n",
    "            nn.Conv2d(dim_ideal * 2, dim_ideal * 4, kernel_size=4, stride=2, padding=1), # 16x16\n",
    "            nn.BatchNorm2d(dim_ideal * 4),\n",
    "            FReLU(dim_ideal * 4)\n",
    "        )\n",
    "        \n",
    "        layers = [\n",
    "            nn.Conv2d(dim_c_code + dim_ideal * 4, dim_ideal * 4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(dim_ideal * 4),\n",
    "            FReLU(dim_ideal * 4)\n",
    "        ]\n",
    "        for _ in range(n_residual):\n",
    "            layers += [ResidualBlock(dim_ideal * 4)]\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        \n",
    "        self.upsample1 = upBlock(dim_ideal * 4, dim_ideal * 2)   # 32x32\n",
    "        self.upsample2 = upBlock(dim_ideal * 2, dim_ideal)       # 64x64\n",
    "        self.upsample3 = upBlock(dim_ideal, dim_ideal // 2)      # 128x128\n",
    "        self.upsample4 = upBlock(dim_ideal // 2, dim_ideal // 4) # 256x256\n",
    "        \n",
    "        self.toRGB = nn.Sequential(\n",
    "            nn.Conv2d(dim_ideal // 4, 3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, image, text):\n",
    "        cond = self.downsample(image)\n",
    "        text = text.view(text.size(0), -1, 1, 1)\n",
    "        c_code = text.repeat(1, 1, 16, 16)\n",
    "        c_code = torch.cat([cond, c_code], 1)\n",
    "        \n",
    "        estimate = self.encoder(c_code)\n",
    "\n",
    "        h_code = self.upsample1(estimate)\n",
    "        h_code = self.upsample2(h_code)\n",
    "        h_code = self.upsample3(h_code)\n",
    "        h_code = self.upsample4(h_code)\n",
    "        fake_img = self.toRGB(h_code)\n",
    "        \n",
    "        noise = torch.randn(c_code.shape).to(c_code.device)\n",
    "        true_pdf = self.encoder(noise).softmax(dim=1)\n",
    "        \n",
    "        # Variational Conditional GAN's Loss\n",
    "        kl_loss = nn.KLDivLoss(reduction='sum')\n",
    "        vc_loss = - kl_loss(estimate.softmax(dim=1), true_pdf)\n",
    "        \n",
    "        return fake_img, vc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STAGE2_D(nn.Module):\n",
    "    def __init__(self, dim_c_code=768, dim_ideal=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        def downBlock(dim_in, dim_out):\n",
    "            block = nn.Sequential(\n",
    "                nn.Conv2d(dim_in, dim_out, kernel_size=4, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(dim_out),\n",
    "                FReLU(dim_out)\n",
    "            )\n",
    "            return block\n",
    "        \n",
    "        self.downconv = nn.Sequential(\n",
    "            downBlock(3, dim_ideal),                   # 128x128\n",
    "            downBlock(dim_ideal, dim_ideal * 2),       # 64x64\n",
    "            downBlock(dim_ideal * 2, dim_ideal * 4),   # 32x32\n",
    "            downBlock(dim_ideal * 4, dim_ideal * 8),   # 16x16\n",
    "            downBlock(dim_ideal * 8, dim_ideal * 16),  # 8x8\n",
    "            downBlock(dim_ideal * 16, dim_ideal * 32), # 4x4\n",
    "            nn.Conv2d(dim_ideal * 32, dim_ideal * 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(dim_ideal * 16),\n",
    "            FReLU(dim_ideal * 16),\n",
    "            nn.Conv2d(dim_ideal * 16, dim_ideal * 8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(dim_ideal * 8),\n",
    "            FReLU(dim_ideal * 8)\n",
    "        )\n",
    "        \n",
    "        self.conv_patch = nn.Conv2d(dim_ideal * 8 + dim_c_code, 1, kernel_size=3, stride=1, padding=1)\n",
    "    \n",
    "    def forward(self, image, text):\n",
    "        cond = self.downconv(image)\n",
    "        \n",
    "        text = text.view(text.size(0), -1, 1, 1)\n",
    "        c_code = text.repeat(1, 1, 4, 4)\n",
    "        \n",
    "        c_code = torch.cat((cond, c_code), 1)\n",
    "        \n",
    "        patch = self.conv_patch(c_code)\n",
    "        return patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    IMG_EXTENSIONS = ['.jpg', '.jpeg', '.png', '.bmp']\n",
    "    \n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.paths = self.get_paths(img_dir)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def get_paths(self, img_dir):\n",
    "        img_dir = Path(img_dir)\n",
    "        paths = [p for p in img_dir.iterdir() if p.suffix in ImageDataset.IMG_EXTENSIONS]\n",
    "        return paths\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        path = self.paths[index]\n",
    "        image = Image.open(path)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextData:\n",
    "    def __init__(self, csv_path):\n",
    "        df = pd.read_csv(csv_path, sep='|')\n",
    "        self.filenames = df.iloc[:,0].tolist()\n",
    "        self.texts = [str(text) for text in df.iloc[:,2].tolist()]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.filenames[index], self.texts[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def tolist(self):\n",
    "        return sum(self.texts, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextAndImageDataset(ImageDataset):\n",
    "    def __init__(self, csv_path, img_dir, transform=None):\n",
    "        super().__init__(img_dir, transform)\n",
    "        self.text_data = TextData(csv_path)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        filename, text = self.text_data[index]\n",
    "        \n",
    "        path = [path for path in self.paths if filename == path.name][0]\n",
    "        image = Image.open(path)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            toTensor = transforms.ToTensor()\n",
    "            image = toTensor(image)\n",
    "        \n",
    "        return image, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomErasing:\n",
    "    def __init__(self, p=0.5, erase_low=0.02, erase_high=0.33, aspect_rl=0.3, aspect_rh=3.3):\n",
    "        self.p = p\n",
    "        self.erase_low = erase_low\n",
    "        self.erase_high = erase_high\n",
    "        self.aspect_rl = aspect_rl\n",
    "        self.aspect_rh = aspect_rh\n",
    "\n",
    "    def __call__(self, image):\n",
    "        if np.random.rand() <= self.p:\n",
    "            c, h, w = image.shape\n",
    "\n",
    "            mask_area = np.random.uniform(self.erase_low, self.erase_high) * (h * w)\n",
    "            mask_aspect_ratio = np.random.uniform(self.aspect_rl, self.aspect_rh)\n",
    "            mask_w = int(np.sqrt(mask_area / mask_aspect_ratio))\n",
    "            mask_h = int(np.sqrt(mask_area * mask_aspect_ratio))\n",
    "\n",
    "            mask = torch.Tensor(np.random.rand(c, mask_h, mask_w) * 255)\n",
    "\n",
    "            left = np.random.randint(0, w)\n",
    "            top = np.random.randint(0, h)\n",
    "            right = left + mask_w\n",
    "            bottom = top + mask_h\n",
    "\n",
    "            if right <= w and bottom <= h:\n",
    "                image[:, top:bottom, left:right] = mask\n",
    "        \n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "meoyoQHI7tAm"
   },
   "outputs": [],
   "source": [
    "class Util:\n",
    "    @staticmethod\n",
    "    def loadImages(batch_size, folder_path, size):\n",
    "        imgs = ImageFolder(folder_path, transform=transforms.Compose([\n",
    "            transforms.Resize(int(size)),\n",
    "            transforms.RandomCrop(size),\n",
    "            transforms.ToTensor()\n",
    "        ]))\n",
    "        return DataLoader(imgs, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver:\n",
    "    def __init__(self, args):\n",
    "        has_cuda = torch.cuda.is_available() if not args.cpu else False\n",
    "        self.device = torch.device(\"cuda\" if has_cuda else \"cpu\")\n",
    "        \n",
    "        self.args = args\n",
    "        \n",
    "        self.load_dataset()\n",
    "        \n",
    "        self.text_encoder = Bert().to(self.device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(Bert.model_name)\n",
    "        self.data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)\n",
    "        \n",
    "        self.stage1_g = STAGE1_G().to(self.device)\n",
    "        self.stage1_d = STAGE1_D().to(self.device)\n",
    "        self.stage2_g = STAGE2_G().to(self.device)\n",
    "        self.stage2_d = STAGE2_D().to(self.device)\n",
    "\n",
    "        self.stage1_g.apply(self.weights_init)\n",
    "        self.stage1_d.apply(self.weights_init)\n",
    "        self.stage2_g.apply(self.weights_init)\n",
    "        self.stage2_d.apply(self.weights_init)\n",
    "\n",
    "        ## DEBUG for check\n",
    "        #print(summary(self.text_encoder))\n",
    "        #pprint(dir(self.text_encoder.model))\n",
    "        #print(summary(self.text_encoder.model.encoder))\n",
    "        for param in self.text_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.text_encoder.model.encoder.layer[-1].parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        self.optimizer_G = optim.Adam([{'params': self.text_encoder.model.encoder.layer[-1].parameters(),\n",
    "                                        'lr': 0.5 * self.args.lr},\n",
    "                                       {'params': itertools.chain(self.stage1_g.parameters(),\n",
    "                                                                  self.stage2_g.parameters()),\n",
    "                                        'lr': 2 * self.args.lr}],\n",
    "                                      betas=(0, 0.9))\n",
    "        self.optimizer_D = optim.Adam([{'params': self.text_encoder.model.encoder.layer[-1].parameters(),\n",
    "                                        'lr': 0.5 * self.args.lr},\n",
    "                                       {'params': itertools.chain(self.stage1_d.parameters(),\n",
    "                                                                  self.stage2_d.parameters()),\n",
    "                                        'lr': 2 * self.args.lr * self.args.mul_lr_dis}],\n",
    "                                      betas=(0, 0.9))\n",
    "        \n",
    "        self.scheduler_G = CosineAnnealingLR(self.optimizer_G, T_max=4, eta_min=self.args.lr/2)\n",
    "        self.scheduler_D = CosineAnnealingLR(self.optimizer_D, T_max=4, eta_min=(self.args.lr * self.args.mul_lr_dis)/2)\n",
    "        \n",
    "        self.pseudo_aug = 0.0\n",
    "        self.epoch = 0\n",
    "    \n",
    "    def weights_init(self, module):\n",
    "        if type(module) == nn.Linear or type(module) == nn.Conv2d or type(module) == nn.ConvTranspose2d:\n",
    "            nn.init.kaiming_normal_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.fill_(0)\n",
    "            \n",
    "    def load_dataset(self, img_size=256):\n",
    "        self.dataset = TextAndImageDataset(self.args.csv_path, self.args.image_dir,\n",
    "                                           transform=transforms.Compose([\n",
    "                                               transforms.Resize(int(img_size)),\n",
    "                                               transforms.RandomCrop(img_size),\n",
    "                                               transforms.ToTensor()\n",
    "                                           ]))\n",
    "        self.dataloader = DataLoader(self.dataset, batch_size=self.args.batch_size,\n",
    "                                     shuffle=True, drop_last=True, num_workers=os.cpu_count())\n",
    "        self.max_iters = len(iter(self.dataloader))\n",
    "\n",
    "    def tokenize(self, texts):\n",
    "        texts = self.tokenizer(texts)\n",
    "        texts = self.data_collator(texts)\n",
    "        return texts\n",
    "            \n",
    "    def save_state(self, epoch):\n",
    "        self.text_encoder.cpu()\n",
    "        self.stage1_g.cpu(), self.stage1_d.cpu(), self.stage2_g.cpu(), self.stage2_d.cpu()\n",
    "        torch.save(self.text_encoder.state_dict(), os.path.join(self.args.weight_dir, f'weight_TE.{epoch}.pth'))\n",
    "        torch.save(self.stage1_g.state_dict(), os.path.join(self.args.weight_dir, f'weight_G1.{epoch}.pth'))\n",
    "        torch.save(self.stage1_d.state_dict(), os.path.join(self.args.weight_dir, f'weight_D1.{epoch}.pth'))\n",
    "        torch.save(self.stage2_g.state_dict(), os.path.join(self.args.weight_dir, f'weight_G2.{epoch}.pth'))\n",
    "        torch.save(self.stage2_d.state_dict(), os.path.join(self.args.weight_dir, f'weight_D2.{epoch}.pth'))\n",
    "        self.text_encoder.to(self.device)\n",
    "        self.stage1_g.to(self.device), self.stage1_d.to(self.device), self.stage2_g.to(self.device), self.stage2_d.to(self.device)\n",
    "        \n",
    "    def load_state(self):\n",
    "        if os.path.exists('weight_TE.pth'):\n",
    "            self.text_encoder.load_state_dict(torch.load('weight_TE.pth', map_location=self.device))\n",
    "            print('Loaded TextEncoder network states.')\n",
    "        if os.path.exists('weight_G1.pth'):\n",
    "            self.stage1_g.load_state_dict(torch.load('weight_G1.pth', map_location=self.device))\n",
    "            print('Loaded Stage1_G network states.')\n",
    "        if os.path.exists('weight_D1.pth'):\n",
    "            self.stage1_d.load_state_dict(torch.load('weight_D1.pth', map_location=self.device))\n",
    "            print('Loaded Stage1_D network states.')\n",
    "        if os.path.exists('weight_G2.pth'):\n",
    "            self.stage2_g.load_state_dict(torch.load('weight_G2.pth', map_location=self.device))\n",
    "            print('Loaded Stage2_G network states.')\n",
    "        if os.path.exists('weight_D2.pth'):\n",
    "            self.stage2_d.load_state_dict(torch.load('weight_D2.pth', map_location=self.device))\n",
    "            print('Loaded Stage2_D network states.')\n",
    "    \n",
    "    def save_resume(self):\n",
    "        with open(os.path.join('.', f'resume.pkl'), 'wb') as f:\n",
    "            dump(self, f)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(args, resume=True):\n",
    "        if resume and os.path.exists('resume.pkl'):\n",
    "            with open(os.path.join('.', 'resume.pkl'), 'rb') as f:\n",
    "                solver = load(f)\n",
    "                print('Loaded resume.')\n",
    "                return solver\n",
    "        else:\n",
    "            return Solver(args)\n",
    "    \n",
    "    def trainGAN(self, epoch, iters, max_iters, real_img, texts, a=0, b=1, c=1):\n",
    "        ### Train with LSGAN.\n",
    "        ### for example, (a, b, c) = 0, 1, 1 or (a, b, c) = -1, 1, 0\n",
    "        \n",
    "        resize_64 = transforms.Resize(64)\n",
    "        resize_256 = transforms.Resize(256)\n",
    "        real_img_64 = resize_64(real_img)\n",
    "        real_img_256 = resize_256(real_img)\n",
    "        \n",
    "        # ================================================================================ #\n",
    "        #                             Train the discriminator                              #\n",
    "        # ================================================================================ #\n",
    "        \n",
    "        text = self.text_encoder(**texts)\n",
    "        text = text.detach()\n",
    "        \n",
    "        fake_img_1, vc_loss_1, noise = self.stage1_g(text)\n",
    "        fake_score_1 = self.stage1_d(fake_img_1, text)\n",
    "        \n",
    "        fake_img_2, vc_loss_2 = self.stage2_g(fake_img_1, text)\n",
    "        fake_score_2 = self.stage2_d(fake_img_2, text)\n",
    "\n",
    "        # for Mode-Seeking\n",
    "        _fake_img_2 = Variable(fake_img_2.data)\n",
    "        _noise = Variable(noise.data)\n",
    "        \n",
    "        real_score_1 = self.stage1_d(real_img_64, text)\n",
    "        real_score_2 = self.stage2_d(real_img_256, text)\n",
    "        \n",
    "        # Compute loss with real images.\n",
    "        real_src_loss = torch.sum((real_score_1 + real_score_2 - b) ** 2)\n",
    "        \n",
    "        # Compute loss with fake images.\n",
    "        p = random.uniform(0, 1)\n",
    "        if 1 - self.pseudo_aug < p:\n",
    "            fake_src_loss = torch.sum((fake_score_1 + fake_score_2 - b) ** 2) # Pseudo: fake is real.\n",
    "        else:\n",
    "            fake_src_loss = torch.sum((fake_score_1 + fake_score_2 - a) ** 2)\n",
    "        \n",
    "        vc_loss = (vc_loss_1 + vc_loss_2) * 1e-5\n",
    "        \n",
    "        # Update Pseudo Augmentation.\n",
    "        lz = (torch.sign(torch.logit(real_score_1 + real_score_2)).mean()\n",
    "              - torch.sign(torch.logit(fake_score_1 + fake_score_2)).mean()) / 2\n",
    "        if lz > 0.6:\n",
    "            self.pseudo_aug += 0.01\n",
    "        else:\n",
    "            self.pseudo_aug -= 0.01\n",
    "        self.pseudo_aug = min(1, max(0, self.pseudo_aug))\n",
    "        \n",
    "        # Backward and optimize.\n",
    "        d_loss = 0.5 * (real_src_loss + fake_src_loss) / self.args.batch_size + vc_loss\n",
    "        self.optimizer_D.zero_grad()\n",
    "        d_loss.backward()\n",
    "        self.optimizer_D.step()\n",
    "        \n",
    "        # Logging.\n",
    "        loss = {}\n",
    "        loss['D/loss'] = d_loss.item()\n",
    "        loss['D/vc_loss'] = vc_loss.item()\n",
    "        loss['D/pseudo_aug'] = self.pseudo_aug\n",
    "        \n",
    "        # ================================================================================ #\n",
    "        #                               Train the generator                                #\n",
    "        # ================================================================================ #\n",
    "        \n",
    "        text = self.text_encoder(**texts)\n",
    "        text = text.detach()\n",
    "        \n",
    "        fake_img_1, vc_loss_1, noise = self.stage1_g(text)\n",
    "        fake_score_1 = self.stage1_d(fake_img_1, text)\n",
    "        \n",
    "        fake_img_2, vc_loss_2 = self.stage2_g(fake_img_1, text)\n",
    "        fake_score_2 = self.stage2_d(fake_img_2, text)\n",
    "        \n",
    "        # Compute loss with fake images.\n",
    "        fake_src_loss = torch.sum((fake_score_1 + fake_score_2 - c) ** 2)\n",
    "        \n",
    "        # Mode Seeking Loss\n",
    "        lz = torch.mean(torch.abs(fake_img_2 - _fake_img_2)) / torch.mean(torch.abs(noise - _noise))\n",
    "        eps = 1 * 1e-5\n",
    "        ms_loss = 1 / (lz + eps)\n",
    "        \n",
    "        # Backward and optimize.\n",
    "        g_loss = 0.5 * fake_src_loss / self.args.batch_size + self.args.lambda_ms * ms_loss\n",
    "        self.optimizer_G.zero_grad()\n",
    "        g_loss.backward()\n",
    "        self.optimizer_G.step()\n",
    "\n",
    "        # Logging.\n",
    "        loss['G/loss'] = g_loss.item()\n",
    "        loss['G/ms_loss'] = ms_loss.item()\n",
    "        \n",
    "        # Save\n",
    "        if iters == max_iters:\n",
    "            self.save_state(epoch)\n",
    "            img_name = str(epoch) + '_' + str(iters) + '_1.png'\n",
    "            img_path = os.path.join(self.args.result_dir, img_name)\n",
    "            save_image(fake_img_1, img_path)\n",
    "            img_name = str(epoch) + '_' + str(iters) + '_2.png'\n",
    "            img_path = os.path.join(self.args.result_dir, img_name)\n",
    "            save_image(fake_img_2, img_path)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def train(self):\n",
    "        print(f'Use Device: {self.device}')\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        \n",
    "        print('Use Scheduler: CosineAnnealingLR')\n",
    "        \n",
    "        self.text_encoder.train()\n",
    "        self.stage1_g.train()\n",
    "        self.stage1_d.train()\n",
    "        self.stage2_g.train()\n",
    "        self.stage2_d.train()\n",
    "        \n",
    "        hyper_params = {}\n",
    "        hyper_params['CSV Path'] = self.args.csv_path\n",
    "        hyper_params['Image Dir'] = self.args.image_dir\n",
    "        hyper_params['Result Dir'] = self.args.result_dir\n",
    "        hyper_params['Weight Dir'] = self.args.weight_dir\n",
    "        hyper_params['Learning Rate'] = self.args.lr\n",
    "        hyper_params[\"Mul Discriminator's LR\"] = self.args.mul_lr_dis\n",
    "        hyper_params['Batch Size'] = self.args.batch_size\n",
    "        hyper_params['Num Train'] = self.args.num_train\n",
    "        hyper_params['Lambda Mode-Seeking'] = self.args.lambda_ms\n",
    "        \n",
    "        for key in hyper_params.keys():\n",
    "            print(f'{key}: {hyper_params[key]}')\n",
    "        #experiment.log_parameters(hyper_params)\n",
    "        \n",
    "        while self.args.num_train > self.epoch:\n",
    "            self.epoch += 1\n",
    "            epoch_loss_G = 0.0\n",
    "            epoch_loss_D = 0.0\n",
    "            \n",
    "            for iters, (images, texts) in enumerate(tqdm(self.dataloader)):\n",
    "                iters += 1\n",
    "                \n",
    "                images = images.to(self.device, non_blocking=True)\n",
    "                texts = self.tokenize(texts).to(self.device)\n",
    "                \n",
    "                loss = self.trainGAN(self.epoch, iters, self.max_iters, images, texts)\n",
    "                \n",
    "                epoch_loss_D += loss['D/loss']\n",
    "                epoch_loss_G += loss['G/loss']\n",
    "                #experiment.log_metrics(loss)\n",
    "            \n",
    "            epoch_loss = epoch_loss_G + epoch_loss_D\n",
    "            \n",
    "            print(f'Epoch[{self.epoch}]'\n",
    "                  + f' LR[G({self.scheduler_G.get_last_lr()[0]:.5f}) D({self.scheduler_D.get_last_lr()[0]:.5f})]'\n",
    "                  + f' G({epoch_loss_G}) + D({epoch_loss_D}) = {epoch_loss}')\n",
    "            \n",
    "            self.scheduler_G.step()\n",
    "            self.scheduler_D.step()\n",
    "            \n",
    "            if not self.args.noresume:\n",
    "                self.save_resume()\n",
    "    \n",
    "    def generate(self, text):\n",
    "        self.text_encoder.eval()\n",
    "        self.stage1_g.eval()\n",
    "        self.stage2_g.eval()\n",
    "        \n",
    "        texts = self.tokenize([text]).to(self.device)\n",
    "        print(self.tokenizer.convert_ids_to_tokens(texts['input_ids'][0].tolist()))\n",
    "        \n",
    "        texts = self.text_encoder(**texts)\n",
    "        fake_img_1, _, _ = self.stage1_g(texts)\n",
    "        fake_img_2, _ = self.stage2_g(fake_img_1, texts)\n",
    "        \n",
    "        save_image(fake_img_1[0], os.path.join(self.args.result_dir, f'generated_1_{time.time()}.png'))\n",
    "        save_image(fake_img_2[0], os.path.join(self.args.result_dir, f'generated_2_{time.time()}.png'))\n",
    "        print('New picture was generated.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    solver = Solver.load(args, resume=not args.noresume)\n",
    "    solver.load_state()\n",
    "    \n",
    "    if args.generate != '':\n",
    "        solver.generate(args.generate)\n",
    "        return\n",
    "    \n",
    "    solver.train()\n",
    "    #experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--csv_path', type=str, default='/mnt/c/Datasets/flickr-images-dataset/results.csv')\n",
    "    parser.add_argument('--image_dir', type=str, default='/mnt/c/Datasets/flickr-images-dataset/flickr30k_images/')\n",
    "    parser.add_argument('--result_dir', type=str, default='results')\n",
    "    parser.add_argument('--weight_dir', type=str, default='weights')\n",
    "    parser.add_argument('--lr', type=float, default=0.0001)\n",
    "    parser.add_argument('--mul_lr_dis', type=float, default=4)\n",
    "    parser.add_argument('--batch_size', type=int, default=8)\n",
    "    parser.add_argument('--num_train', type=int, default=100)\n",
    "    parser.add_argument('--lambda_ms', type=float, default=1)\n",
    "    parser.add_argument('--cpu', action='store_true')\n",
    "    parser.add_argument('--noresume', action='store_true')\n",
    "    parser.add_argument('--generate', type=str, default='')\n",
    "    \n",
    "    args, unknown = parser.parse_known_args()\n",
    "    \n",
    "    if not os.path.exists(args.result_dir):\n",
    "        os.mkdir(args.result_dir)\n",
    "    if not os.path.exists(args.weight_dir):\n",
    "        os.mkdir(args.weight_dir)\n",
    "    \n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CycleGAN.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
