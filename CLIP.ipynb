{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from comet_ml import Experiment\n",
    "#experiment = Experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Ccpy4OkFMEM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import cv2\n",
    "import time\n",
    "import argparse\n",
    "import itertools\n",
    "import string\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageFile\n",
    "from pickle import load, dump\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "#import torchtext\n",
    "#if 'legacy' in dir(torchtext):\n",
    "#    import torchtext.legacy as torchtext\n",
    "\n",
    "from torch import einsum\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mish(nn.Module):\n",
    "    @staticmethod\n",
    "    def mish(x):\n",
    "        return x * torch.tanh(F.softplus(x))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return Mish.mish(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "        \n",
    "        inner_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if not (heads == 1 and dim_head == dim) else nn.Identity()\n",
    "        \n",
    "    def forward(self, x, return_attention=False):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        query, key, value = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "        \n",
    "        attention_score = torch.matmul(query, key.transpose(-1, -2)) * self.scale\n",
    "        attention_prob = F.softmax(attention_score, dim=-1)\n",
    "        attention_prob = self.dropout(attention_prob)\n",
    "        \n",
    "        context = torch.matmul(attention_prob, value)\n",
    "        context = rearrange(context, 'b h n d -> b n (h d)')\n",
    "        \n",
    "        if return_attention:\n",
    "            return self.to_out(context), attention_prob\n",
    "        else:\n",
    "            return self.to_out(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            Mish(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "        \n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, mlp_dim, depth=4, heads=8, dim_head=64, dropout=0.):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout))\n",
    "            ]))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        for attention, feedforward in self.layers:\n",
    "            x = attention(x) + x\n",
    "            x = feedforward(x) + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoolFormer(nn.Module):\n",
    "    def __init__(self, dim, mlp_dim, pool_size=3, depth=4, heads=8, dim_head=64, dropout=0.):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, nn.AvgPool2d(pool_size, stride=1, padding=pool_size//2, count_include_pad=False)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout))\n",
    "            ]))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        for pooling, feedforward in self.layers:\n",
    "            x = pooling(x) + x\n",
    "            x = feedforward(x) + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, sentence_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.pe = torch.Tensor(sentence_size, vocab_size)\n",
    "        for pos in range(sentence_size):\n",
    "            for i in range(0, vocab_size, 2):\n",
    "                self.pe[pos, i] = math.sin(pos / (10000**((2*i)/vocab_size)))\n",
    "                self.pe[pos, i+1] = math.cos(pos / (10000**((2*(i+1))/vocab_size)))\n",
    "        self.pe = self.pe.unsqueeze(0)\n",
    "        self.pe.requires_grad = False\n",
    "    \n",
    "    def to(self, device):\n",
    "        self.pe = self.pe.to(device)\n",
    "        return super().to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return math.sqrt(self.vocab_size) * x + self.pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, sentence_size, dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.word_embedding = nn.Embedding(vocab_size, dim)\n",
    "        #self.position_embedding = nn.Embedding(sentence_size, dim)\n",
    "        self.position_embedding = PositionalEncoder(dim, sentence_size)\n",
    "        self.LayerNorm = nn.LayerNorm(dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        word_embedding = self.word_embedding(input_ids)\n",
    "        \n",
    "        #position_ids = torch.arange(input_ids.size(1), dtype=torch.long, device=input_ids.device)\n",
    "        #position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        #position_embedding = self.position_embedding(position_ids)\n",
    "        \n",
    "        #embedding = word_embedding + position_embedding\n",
    "        embedding = self.position_embedding(word_embedding)\n",
    "        embedding = self.LayerNorm(embedding)\n",
    "        embedding = self.dropout(embedding)\n",
    "        \n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, sentence_size, dim_out=128, dim=512, mlp_dim=1024,\n",
    "                 pool='cls', channels=3, dropout=0., emb_dropout=0.):\n",
    "        super().__init__()\n",
    "\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "        self.pool = pool\n",
    "        \n",
    "        self.embedding = Embedding(vocab_size, sentence_size, dim, emb_dropout)\n",
    "        self.transformer = Transformer(dim, mlp_dim, dropout=dropout)\n",
    "        \n",
    "        self.f = nn.Identity()\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, dim_out)\n",
    "        )\n",
    "    \n",
    "    def to(self, *args, **kwargs):\n",
    "        self.embedding.position_embedding.to(args[0])\n",
    "        return super().to(*args, **kwargs)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "\n",
    "        x = self.f(x)\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, max_patches, patch_size, num_classes, dim=512, mlp_dim=1024,\n",
    "                 pool='cls', channels=3, dropout=0., emb_dropout=0.):\n",
    "        super().__init__()\n",
    "        \n",
    "        def pair(t):\n",
    "            return t if isinstance(t, tuple) else (t, t)\n",
    "        \n",
    "        self.patch_height, self.patch_width = pair(patch_size)\n",
    "        patch_dim = channels * self.patch_height * self.patch_width\n",
    "        \n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "        self.pool = pool\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        \n",
    "        self.patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = self.patch_height, p2 = self.patch_width),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "        )\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, max_patches + 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "        \n",
    "        #self.transformer = Transformer(dim, mlp_dim, dropout=dropout)\n",
    "        self.transformer = PoolFormer(dim, mlp_dim, dropout=dropout)\n",
    "        \n",
    "        self.f = nn.Identity()\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        h, w = img.size(2), img.size(3)\n",
    "        resize = transforms.Resize((h - h % self.patch_height, w - w % self.patch_width))\n",
    "        img = resize(img)\n",
    "        \n",
    "        x = self.patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        \n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "\n",
    "        x = self.f(x)\n",
    "        \n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP(nn.Module):\n",
    "    def __init__(self, max_patches, patch_size, vocab_size, sentence_size, img_classes=128, text_classes=128, dim_embed=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.image_encoder = VisionTransformer(max_patches, patch_size, img_classes)\n",
    "        self.text_encoder = TextTransformer(vocab_size, sentence_size, text_classes)\n",
    "        self.weight_image = nn.Parameter(torch.randn(img_classes, dim_embed))\n",
    "        self.weight_text = nn.Parameter(torch.randn(text_classes, dim_embed))\n",
    "        self.logit_scale = nn.Parameter(torch.randn(1))\n",
    "    \n",
    "    def to(self, *args, **kwargs):\n",
    "        self.text_encoder.to(args[0])\n",
    "        return super().to(*args, **kwargs)\n",
    "    \n",
    "    def forward(self, image, text):\n",
    "        img = self.image_encoder(image)\n",
    "        text = self.text_encoder(text)\n",
    "        \n",
    "        image_norm = torch.mm(img, self.weight_image).norm(dim=1, keepdim=True)\n",
    "        text_norm = torch.mm(text, self.weight_text).norm(dim=1, keepdim=True)\n",
    "        \n",
    "        logits = torch.mm(image_norm, text_norm.T) * self.logit_scale.exp()\n",
    "        \n",
    "        n = logits.size(0)\n",
    "        labels = torch.arange(n).to(logits.device)\n",
    "        loss_image = F.cross_entropy(logits, labels)\n",
    "        loss_text = F.cross_entropy(logits.T, labels)\n",
    "        loss = (loss_image + loss_text) / 2\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.paths = self.get_paths(img_dir)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def get_paths(self, img_dir):\n",
    "        paths = []\n",
    "        print('Make image-path of directories.')\n",
    "        for root, dirs, files in tqdm(os.walk(img_dir)):\n",
    "            for file in files:\n",
    "                paths += [Path(os.path.join(root, file))]\n",
    "        return paths\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        path = self.paths[index]\n",
    "        image = Image.open(path)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset:\n",
    "    def __init__(self, text_dir):\n",
    "        self.filenames = []\n",
    "        self.texts = []\n",
    "        print('Make text-data of directories.')\n",
    "        for root, dirs, files in tqdm(os.walk(text_dir)):\n",
    "            for file in files:\n",
    "                filename = os.path.splitext(file)\n",
    "                if filename[1] == '.txt':\n",
    "                    with open(os.path.join(root, file), 'r') as f:\n",
    "                        self.filenames += [filename[0] + '.jpg']\n",
    "                        self.texts += [TextDataset.tokenizer(f.read())]\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenizer(text):\n",
    "        for s in string.punctuation:\n",
    "            if not(s == '.' or s == ','):\n",
    "                text = text.replace(s, ' ')\n",
    "            else:\n",
    "                text = text.replace(s, ' ' + s + ' ')\n",
    "        text = text.strip().split()\n",
    "        return text\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.filenames[index], self.texts[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def tolist(self):\n",
    "        return sum(self.texts, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextAndImageDataset(ImageDataset):\n",
    "    @staticmethod\n",
    "    def make_vocab(textdata: TextDataset, vocab_size=None):\n",
    "        print('Generate word-ids.')\n",
    "        word2id = {}\n",
    "        word2id['<pad>'] = 0\n",
    "        word2id['<unk>'] = 1\n",
    "        \n",
    "        #wc = collections.Counter(textdata.tolist())\n",
    "        #for i, (w, _) in enumerate(wc.most_common(vocab_size), 2):\n",
    "        #    word2id[w] = i\n",
    "        \n",
    "        id2word = {v: k for k, v in word2id.items()}\n",
    "\n",
    "        for texts in tqdm(textdata):\n",
    "            for words in texts:\n",
    "                for word in words:\n",
    "                    if word not in word2id:\n",
    "                        id = len(word2id)\n",
    "                        word2id[word] = id\n",
    "                        id2word[id] = word\n",
    "        \n",
    "        return word2id, id2word\n",
    "    \n",
    "    def __init__(self, text_dir, img_dir, sentence_size, vocab_size=None, transform=None):\n",
    "        super().__init__(img_dir, transform)\n",
    "        \n",
    "        self.sentence_size = sentence_size\n",
    "        \n",
    "        if os.path.exists('textdata.dat'):\n",
    "            with open(os.path.join('.', 'textdata.dat'), 'rb') as f:\n",
    "                self.textdata = load(f)\n",
    "                print('Loaded textdata.dat.')\n",
    "        else:\n",
    "            self.textdata = TextDataset(text_dir)\n",
    "            with open(os.path.join('.', 'textdata.dat'), 'wb') as f:\n",
    "                dump(self.textdata, f)\n",
    "                print('Saved textdata.dat.')\n",
    "        \n",
    "        if os.path.exists('word2id.dat') and os.path.exists('id2word.dat'):\n",
    "            with open(os.path.join('.', 'word2id.dat'), 'rb') as f:\n",
    "                self.word2id = load(f)\n",
    "                print('Loaded word2id.dat.')\n",
    "            with open(os.path.join('.', 'id2word.dat'), 'rb') as f:\n",
    "                self.id2word = load(f)\n",
    "                print('Loaded id2word.dat.')\n",
    "        else:\n",
    "            self.word2id, self.id2word = TextAndImageDataset.make_vocab(self.textdata, vocab_size)\n",
    "            with open(os.path.join('.', 'word2id.dat'), 'wb') as f:\n",
    "                dump(self.word2id, f)\n",
    "                print('Saved word2id.dat.')\n",
    "            with open(os.path.join('.', 'id2word.dat'), 'wb') as f:\n",
    "                dump(self.id2word, f)\n",
    "                print('Saved id2word.dat.')\n",
    "        \n",
    "    \n",
    "    def to_string(self, tensor):\n",
    "        text = ''\n",
    "        for d in tensor.tolist():\n",
    "            text += self.id2word[d] + ' '\n",
    "        return text\n",
    "    \n",
    "    def to_tokens(self, data):\n",
    "        tokens = []\n",
    "        for d in data:\n",
    "            tokens += [self.word2id[d] if d in self.word2id else self.word2id['<unk>']]\n",
    "        return tokens\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        filename, text = self.textdata[index]\n",
    "        \n",
    "        path = [path for path in self.paths if filename == path.name][0]\n",
    "        image = Image.open(path)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            toTensor = transforms.ToTensor()\n",
    "            image = toTensor(image)\n",
    "        \n",
    "        text = self.to_tokens(text)\n",
    "        text.extend([self.word2id['<pad>'] for _ in range(self.sentence_size - len(text))])\n",
    "        text = torch.LongTensor(text)\n",
    "        \n",
    "        return image, text\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.textdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver:\n",
    "    def __init__(self, args):\n",
    "        has_cuda = torch.cuda.is_available() if not args.cpu else False\n",
    "        self.device = torch.device(\"cuda\" if has_cuda else \"cpu\")\n",
    "        \n",
    "        self.args = args\n",
    "        \n",
    "        self.load_dataset()\n",
    "        \n",
    "        self.CLIP = CLIP(self.args.CLIP_max_patches, self.args.CLIP_patch_size,\n",
    "                         len(self.dataset.word2id), self.args.CLIP_sentence_size).to(self.device)\n",
    "        self.CLIP.apply(self.weights_init)\n",
    "        self.optimizer_CLIP = optim.Adam(self.CLIP.parameters(), lr=self.args.lr, betas=(0, 0.9))\n",
    "        \n",
    "        self.epoch = 0\n",
    "    \n",
    "    def weights_init(self, module):\n",
    "        if type(module) == nn.Linear or type(module) == nn.Conv2d or type(module) == nn.ConvTranspose2d:\n",
    "            nn.init.kaiming_normal_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.fill_(0)\n",
    "            \n",
    "    def load_dataset(self, img_size=256):\n",
    "        self.dataset = TextAndImageDataset(self.args.text_dir, self.args.image_dir, self.args.CLIP_sentence_size,\n",
    "                                           transform=transforms.Compose([\n",
    "                                               transforms.Resize(int(img_size)),\n",
    "                                               transforms.RandomCrop(img_size),\n",
    "                                               transforms.RandomHorizontalFlip(),\n",
    "                                               transforms.ToTensor()\n",
    "                                           ]))\n",
    "        self.dataloader = DataLoader(self.dataset, batch_size=self.args.batch_size,\n",
    "                                     shuffle=True, drop_last=True, num_workers=os.cpu_count())\n",
    "        self.max_iters = len(iter(self.dataloader))\n",
    "            \n",
    "    def save_state(self, epoch):\n",
    "        self.CLIP.cpu()\n",
    "        torch.save(self.CLIP.state_dict(), os.path.join(self.args.weight_dir, f'weight_CLIP.{epoch}.pth'))\n",
    "        self.CLIP.to(self.device)\n",
    "        \n",
    "    def load_state(self):\n",
    "        if os.path.exists('weight_CLIP.pth'):\n",
    "            self.CLIP.load_state_dict(torch.load('weight_CLIP.pth', map_location=self.device))\n",
    "            print('Loaded CLIP network state.')\n",
    "    \n",
    "    def save_resume(self):\n",
    "        with open(os.path.join('.', f'resume.pkl'), 'wb') as f:\n",
    "            dump(self, f)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load(args, resume=False):\n",
    "        if resume and os.path.exists('resume.pkl'):\n",
    "            with open(os.path.join('.', 'resume.pkl'), 'rb') as f:\n",
    "                print('Load resume.')\n",
    "                solver = load(f)\n",
    "                solver.args = args\n",
    "                return solver\n",
    "        else:\n",
    "            return Solver(args)\n",
    "    \n",
    "    def trainCLIP(self, epoch, iters, max_iters, images, texts):\n",
    "        loss = self.CLIP(images, texts)\n",
    "        \n",
    "        self.optimizer_CLIP.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer_CLIP.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def train(self):\n",
    "        print(f'Use Device: {self.device}')\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        \n",
    "        self.CLIP.train()\n",
    "        \n",
    "        hyper_params = {}\n",
    "        hyper_params['Text Dir'] = self.args.text_dir\n",
    "        hyper_params['Image Dir'] = self.args.image_dir\n",
    "        hyper_params['Result Dir'] = self.args.result_dir\n",
    "        hyper_params['Weight Dir'] = self.args.weight_dir\n",
    "        hyper_params['CLIP_max_patches'] = self.args.CLIP_max_patches\n",
    "        hyper_params['CLIP_patch_size'] = self.args.CLIP_patch_size\n",
    "        hyper_params['CLIP_sentence_size'] = self.args.CLIP_sentence_size\n",
    "        hyper_params['Learning Rate'] = self.args.lr\n",
    "        hyper_params['Batch Size'] = self.args.batch_size\n",
    "        hyper_params['Num Train'] = self.args.num_train\n",
    "        \n",
    "        for key in hyper_params.keys():\n",
    "            print(f'{key}: {hyper_params[key]}')\n",
    "        #experiment.log_parameters(hyper_params)\n",
    "        \n",
    "        while self.args.num_train > self.epoch:\n",
    "            self.epoch += 1\n",
    "            epoch_loss_CLIP = 0.0\n",
    "            \n",
    "            for iters, (images, texts) in enumerate(tqdm(self.dataloader)):\n",
    "                iters += 1\n",
    "                loss = {}\n",
    "                \n",
    "                images = images.to(self.device, non_blocking=True)\n",
    "                texts = texts.to(self.device, non_blocking=True)\n",
    "                \n",
    "                loss_CLIP = self.trainCLIP(self.epoch, iters, self.max_iters, images, texts)\n",
    "                loss['CLIP/loss'] = loss_CLIP\n",
    "                \n",
    "                epoch_loss_CLIP += loss['CLIP/loss']\n",
    "                #experiment.log_metrics(loss)\n",
    "            \n",
    "            epoch_loss = epoch_loss_CLIP\n",
    "            \n",
    "            print(f'Epoch[{self.epoch}]'\n",
    "                  + f' + Loss[CLIP({epoch_loss_CLIP}) = {epoch_loss}]')\n",
    "            \n",
    "            self.save_state('last')\n",
    "                    \n",
    "            if not self.args.noresume:\n",
    "                self.save_resume()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    solver = Solver.load(args, resume=not args.noresume)\n",
    "    solver.load_state()\n",
    "    \n",
    "    solver.train()\n",
    "    #experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--text_dir', type=str, default='/usr/share/datasets/cub2002011/cvpr2016_cub/text_c10/')\n",
    "    parser.add_argument('--image_dir', type=str, default='/usr/share/datasets/cub2002011/CUB_200_2011/images/')\n",
    "    parser.add_argument('--result_dir', type=str, default='results')\n",
    "    parser.add_argument('--weight_dir', type=str, default='weights')\n",
    "    parser.add_argument('--CLIP_max_patches', type=int, default=128)\n",
    "    parser.add_argument('--CLIP_patch_size', type=int, default=32)\n",
    "    parser.add_argument('--CLIP_sentence_size', type=int, default=512)\n",
    "    parser.add_argument('--lr', type=float, default=0.00001)\n",
    "    parser.add_argument('--batch_size', type=int, default=8)\n",
    "    parser.add_argument('--num_train', type=int, default=10)\n",
    "    parser.add_argument('--cpu', action='store_true')\n",
    "    parser.add_argument('--noresume', action='store_true')\n",
    "    \n",
    "    args, unknown = parser.parse_known_args()\n",
    "    \n",
    "    if not os.path.exists(args.result_dir):\n",
    "        os.mkdir(args.result_dir)\n",
    "    if not os.path.exists(args.weight_dir):\n",
    "        os.mkdir(args.weight_dir)\n",
    "    \n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CycleGAN.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
