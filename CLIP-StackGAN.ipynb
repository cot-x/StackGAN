{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from comet_ml import Experiment\n",
    "#experiment = Experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Ccpy4OkFMEM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import cv2\n",
    "import time\n",
    "import argparse\n",
    "import itertools\n",
    "import string\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageFile\n",
    "from pickle import load, dump\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "#import torchtext\n",
    "#if 'legacy' in dir(torchtext):\n",
    "#    import torchtext.legacy as torchtext\n",
    "    \n",
    "from torch import einsum\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FReLU(nn.Module):\n",
    "    def __init__(self, n_channel, kernel=3, stride=1, padding=1):\n",
    "        super().__init__()\n",
    "        self.funnel_condition = nn.Conv2d(n_channel, n_channel, kernel_size=kernel,stride=stride, padding=padding, groups=n_channel)\n",
    "        self.normalize = nn.BatchNorm2d(n_channel)\n",
    "\n",
    "    def forward(self, x):\n",
    "        tx = self.normalize(self.funnel_condition(x))\n",
    "        out = torch.max(x, tx)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(in_features),\n",
    "            FReLU(in_features),\n",
    "            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(in_features)\n",
    "        )\n",
    "        self.activate = FReLU(in_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = self.shortcut(x)\n",
    "        return self.activate(self.residual(x) + shortcut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "        \n",
    "        inner_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if not (heads == 1 and dim_head == dim) else nn.Identity()\n",
    "        \n",
    "    def forward(self, x, return_attention=False):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        query, key, value = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "        \n",
    "        attention_score = torch.matmul(query, key.transpose(-1, -2)) * self.scale\n",
    "        attention_prob = F.softmax(attention_score, dim=-1)\n",
    "        attention_prob = self.dropout(attention_prob)\n",
    "        \n",
    "        context = torch.matmul(attention_prob, value)\n",
    "        context = rearrange(context, 'b h n d -> b n (h d)')\n",
    "        \n",
    "        if return_attention:\n",
    "            return self.to_out(context), attention_prob\n",
    "        else:\n",
    "            return self.to_out(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.Mish(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "        \n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, mlp_dim, depth=4, heads=8, dim_head=64, dropout=0.):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout))\n",
    "            ]))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        for attention, feedforward in self.layers:\n",
    "            x = attention(x) + x\n",
    "            x = feedforward(x) + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoolFormer(nn.Module):\n",
    "    def __init__(self, dim, mlp_dim, pool_size=3, depth=4, heads=8, dim_head=64, dropout=0.):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, nn.AvgPool2d(pool_size, stride=1, padding=pool_size//2, count_include_pad=False)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout))\n",
    "            ]))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        for pooling, feedforward in self.layers:\n",
    "            x = pooling(x) + x\n",
    "            x = feedforward(x) + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, sentence_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.pe = torch.Tensor(sentence_size, vocab_size)\n",
    "        for pos in range(sentence_size):\n",
    "            for i in range(0, vocab_size, 2):\n",
    "                self.pe[pos, i] = math.sin(pos / (10000**((2*i)/vocab_size)))\n",
    "                self.pe[pos, i+1] = math.cos(pos / (10000**((2*(i+1))/vocab_size)))\n",
    "        self.pe = self.pe.unsqueeze(0)\n",
    "        self.pe.requires_grad = False\n",
    "    \n",
    "    def to(self, device):\n",
    "        self.pe = self.pe.to(device)\n",
    "        return super().to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return math.sqrt(self.vocab_size) * x + self.pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, sentence_size, dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.word_embedding = nn.Embedding(vocab_size, dim)\n",
    "        #self.position_embedding = nn.Embedding(sentence_size, dim)\n",
    "        self.position_embedding = PositionalEncoder(dim, sentence_size)\n",
    "        self.LayerNorm = nn.LayerNorm(dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        word_embedding = self.word_embedding(input_ids)\n",
    "        \n",
    "        #position_ids = torch.arange(input_ids.size(1), dtype=torch.long, device=input_ids.device)\n",
    "        #position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        #position_embedding = self.position_embedding(position_ids)\n",
    "        \n",
    "        #embedding = word_embedding + position_embedding\n",
    "        embedding = self.position_embedding(word_embedding)\n",
    "        embedding = self.LayerNorm(embedding)\n",
    "        embedding = self.dropout(embedding)\n",
    "        \n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, sentence_size, dim_out=128, dim=512, mlp_dim=1024,\n",
    "                 pool='cls', channels=3, dropout=0., emb_dropout=0.):\n",
    "        super().__init__()\n",
    "\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "        self.pool = pool\n",
    "        \n",
    "        self.embedding = Embedding(vocab_size, sentence_size, dim, emb_dropout)\n",
    "        self.transformer = Transformer(dim, mlp_dim, dropout=dropout)\n",
    "        \n",
    "        self.f = nn.Identity()\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, dim_out)\n",
    "        )\n",
    "    \n",
    "    def to(self, *args, **kwargs):\n",
    "        self.embedding.position_embedding.to(args[0])\n",
    "        return super().to(*args, **kwargs)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "\n",
    "        x = self.f(x)\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, max_patches, patch_size, num_classes, dim=512, mlp_dim=1024,\n",
    "                 pool='cls', channels=3, dropout=0., emb_dropout=0.):\n",
    "        super().__init__()\n",
    "        \n",
    "        def pair(t):\n",
    "            return t if isinstance(t, tuple) else (t, t)\n",
    "        \n",
    "        self.patch_height, self.patch_width = pair(patch_size)\n",
    "        patch_dim = channels * self.patch_height * self.patch_width\n",
    "        \n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "        self.pool = pool\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        \n",
    "        self.patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = self.patch_height, p2 = self.patch_width),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "        )\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, max_patches + 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "        \n",
    "        #self.transformer = Transformer(dim, mlp_dim, dropout=dropout)\n",
    "        self.transformer = PoolFormer(dim, mlp_dim, dropout=dropout)\n",
    "        \n",
    "        self.f = nn.Identity()\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        h, w = img.size(2), img.size(3)\n",
    "        resize = transforms.Resize((h - h % self.patch_height, w - w % self.patch_width))\n",
    "        img = resize(img)\n",
    "        \n",
    "        x = self.patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        \n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "\n",
    "        x = self.f(x)\n",
    "        \n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP(nn.Module):\n",
    "    def __init__(self, max_patches, patch_size, vocab_size, sentence_size, img_classes=128, text_classes=128, dim_embed=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.image_encoder = VisionTransformer(max_patches, patch_size, img_classes)\n",
    "        self.text_encoder = TextTransformer(vocab_size, sentence_size, text_classes)\n",
    "        self.weight_image = nn.Parameter(torch.randn(img_classes, dim_embed))\n",
    "        self.weight_text = nn.Parameter(torch.randn(text_classes, dim_embed))\n",
    "        self.logit_scale = nn.Parameter(torch.randn(1))\n",
    "    \n",
    "    def to(self, *args, **kwargs):\n",
    "        self.text_encoder.to(args[0])\n",
    "        return super().to(*args, **kwargs)\n",
    "    \n",
    "    def forward(self, image, text):\n",
    "        img = self.image_encoder(image)\n",
    "        text = self.text_encoder(text)\n",
    "        \n",
    "        image_norm = torch.mm(img, self.weight_image).norm(dim=1, keepdim=True)\n",
    "        text_norm = torch.mm(text, self.weight_text).norm(dim=1, keepdim=True)\n",
    "        \n",
    "        logits = torch.mm(image_norm, text_norm.T) * self.logit_scale.exp()\n",
    "        \n",
    "        n = logits.size(0)\n",
    "        labels = torch.arange(n).to(logits.device)\n",
    "        loss_image = F.cross_entropy(logits, labels)\n",
    "        loss_text = F.cross_entropy(logits.T, labels)\n",
    "        loss = (loss_image + loss_text) / 2\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STAGE1_G(nn.Module):\n",
    "    def __init__(self, dim_c_code=128, dim_noise=128, dim_ideal=1024):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dim_noise = dim_noise\n",
    "        \n",
    "        def upBlock(dim_in, dim_out):\n",
    "            block = nn.Sequential(\n",
    "                nn.Upsample(scale_factor=2),\n",
    "                nn.Conv2d(dim_in, dim_out, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(dim_out),\n",
    "                FReLU(dim_out)\n",
    "            )\n",
    "            return block\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(dim_c_code + dim_noise, dim_ideal * 4 * 4),\n",
    "            nn.Mish(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.upsample1 = upBlock(dim_ideal, dim_ideal // 2)       # 8x8\n",
    "        self.upsample2 = upBlock(dim_ideal // 2, dim_ideal // 4)  # 16x16\n",
    "        self.upsample3 = upBlock(dim_ideal // 4, dim_ideal // 8)  # 32x32\n",
    "        self.upsample4 = upBlock(dim_ideal // 8, dim_ideal // 16) # 64x64\n",
    "        \n",
    "        self.toRGB = nn.Sequential(\n",
    "            nn.Conv2d(dim_ideal // 16, 3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, text):\n",
    "        noise = torch.randn(text.size(0), self.dim_noise).to(text.device)\n",
    "        c_code = torch.cat((noise, text), 1)\n",
    "        \n",
    "        estimate = self.encoder(c_code)\n",
    "        \n",
    "        h_code = estimate.view(estimate.size(0), -1, 4, 4)\n",
    "        h_code = self.upsample1(h_code)\n",
    "        h_code = self.upsample2(h_code)\n",
    "        h_code = self.upsample3(h_code)\n",
    "        h_code = self.upsample4(h_code)\n",
    "        fake_img = self.toRGB(h_code)\n",
    "        \n",
    "        _noise = torch.randn(c_code.shape).to(text.device)\n",
    "        true_pdf = self.encoder(_noise).softmax(dim=1)\n",
    "        \n",
    "        # Variational Conditional GAN's Loss\n",
    "        kl_loss = nn.KLDivLoss(reduction='sum')\n",
    "        vc_loss = - kl_loss(estimate.softmax(dim=1), true_pdf)\n",
    "        \n",
    "        return fake_img, vc_loss, noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STAGE1_D(nn.Module):\n",
    "    def __init__(self, dim_c_code=128, dim_ideal=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        def downBlock(dim_in, dim_out):\n",
    "            block = nn.Sequential(\n",
    "                nn.Conv2d(dim_in, dim_out, kernel_size=4, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(dim_out),\n",
    "                FReLU(dim_out)\n",
    "            )\n",
    "            return block\n",
    "        \n",
    "        self.downconv = nn.Sequential(\n",
    "            downBlock(3, dim_ideal),                 # 32x32\n",
    "            downBlock(dim_ideal, dim_ideal * 2),     # 16x16\n",
    "            downBlock(dim_ideal * 2, dim_ideal * 4), # 8x8\n",
    "            downBlock(dim_ideal * 4, dim_ideal * 8)  # 4x4\n",
    "        )\n",
    "        \n",
    "        self.conv_patch = nn.Conv2d(dim_ideal * 8 + dim_c_code, 1, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "    def forward(self, image, text):\n",
    "        cond = self.downconv(image)\n",
    "        \n",
    "        text = text.view(text.size(0), -1, 1, 1)\n",
    "        c_code = text.repeat(1, 1, 4, 4)\n",
    "        \n",
    "        c_code = torch.cat((cond, c_code), 1)\n",
    "        \n",
    "        patch = self.conv_patch(c_code)\n",
    "        return patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STAGE2_G(nn.Module):\n",
    "    def __init__(self, dim_c_code=128, dim_ideal=128, n_residual=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        def upBlock(dim_in, dim_out):\n",
    "            block = nn.Sequential(\n",
    "                nn.Upsample(scale_factor=2),\n",
    "                nn.Conv2d(dim_in, dim_out, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(dim_out),\n",
    "                FReLU(dim_out)\n",
    "            )\n",
    "            return block\n",
    "        \n",
    "        self.downsample = nn.Sequential(\n",
    "            nn.Conv2d(3, dim_ideal, kernel_size=3, stride=1, padding=1),\n",
    "            FReLU(dim_ideal),\n",
    "            nn.Conv2d(dim_ideal, dim_ideal * 2, kernel_size=4, stride=2, padding=1),     # 32x32\n",
    "            nn.BatchNorm2d(dim_ideal * 2),\n",
    "            FReLU(dim_ideal * 2),\n",
    "            nn.Conv2d(dim_ideal * 2, dim_ideal * 4, kernel_size=4, stride=2, padding=1), # 16x16\n",
    "            nn.BatchNorm2d(dim_ideal * 4),\n",
    "            FReLU(dim_ideal * 4)\n",
    "        )\n",
    "        \n",
    "        layers = [\n",
    "            nn.Conv2d(dim_c_code + dim_ideal * 4, dim_ideal * 4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(dim_ideal * 4),\n",
    "            FReLU(dim_ideal * 4)\n",
    "        ]\n",
    "        for _ in range(n_residual):\n",
    "            layers += [ResidualBlock(dim_ideal * 4)]\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        \n",
    "        self.upsample1 = upBlock(dim_ideal * 4, dim_ideal * 2)   # 32x32\n",
    "        self.upsample2 = upBlock(dim_ideal * 2, dim_ideal)       # 64x64\n",
    "        self.upsample3 = upBlock(dim_ideal, dim_ideal // 2)      # 128x128\n",
    "        self.upsample4 = upBlock(dim_ideal // 2, dim_ideal // 4) # 256x256\n",
    "        \n",
    "        self.toRGB = nn.Sequential(\n",
    "            nn.Conv2d(dim_ideal // 4, 3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, image, text):\n",
    "        cond = self.downsample(image)\n",
    "        text = text.view(text.size(0), -1, 1, 1)\n",
    "        c_code = text.repeat(1, 1, 16, 16)\n",
    "        c_code = torch.cat([cond, c_code], 1)\n",
    "        \n",
    "        estimate = self.encoder(c_code)\n",
    "\n",
    "        h_code = self.upsample1(estimate)\n",
    "        h_code = self.upsample2(h_code)\n",
    "        h_code = self.upsample3(h_code)\n",
    "        h_code = self.upsample4(h_code)\n",
    "        fake_img = self.toRGB(h_code)\n",
    "        \n",
    "        noise = torch.randn(c_code.shape).to(c_code.device)\n",
    "        true_pdf = self.encoder(noise).softmax(dim=1)\n",
    "        \n",
    "        # Variational Conditional GAN's Loss\n",
    "        kl_loss = nn.KLDivLoss(reduction='sum')\n",
    "        vc_loss = - kl_loss(estimate.softmax(dim=1), true_pdf)\n",
    "        \n",
    "        return fake_img, vc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STAGE2_D(nn.Module):\n",
    "    def __init__(self, dim_c_code=128, dim_ideal=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        def downBlock(dim_in, dim_out):\n",
    "            block = nn.Sequential(\n",
    "                nn.Conv2d(dim_in, dim_out, kernel_size=4, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(dim_out),\n",
    "                FReLU(dim_out)\n",
    "            )\n",
    "            return block\n",
    "        \n",
    "        self.downconv = nn.Sequential(\n",
    "            downBlock(3, dim_ideal),                   # 128x128\n",
    "            downBlock(dim_ideal, dim_ideal * 2),       # 64x64\n",
    "            downBlock(dim_ideal * 2, dim_ideal * 4),   # 32x32\n",
    "            downBlock(dim_ideal * 4, dim_ideal * 8),   # 16x16\n",
    "            downBlock(dim_ideal * 8, dim_ideal * 16),  # 8x8\n",
    "            downBlock(dim_ideal * 16, dim_ideal * 32), # 4x4\n",
    "            nn.Conv2d(dim_ideal * 32, dim_ideal * 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(dim_ideal * 16),\n",
    "            FReLU(dim_ideal * 16),\n",
    "            nn.Conv2d(dim_ideal * 16, dim_ideal * 8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(dim_ideal * 8),\n",
    "            FReLU(dim_ideal * 8)\n",
    "        )\n",
    "        \n",
    "        self.conv_patch = nn.Conv2d(dim_ideal * 8 + dim_c_code, 1, kernel_size=3, stride=1, padding=1)\n",
    "    \n",
    "    def forward(self, image, text):\n",
    "        cond = self.downconv(image)\n",
    "        \n",
    "        text = text.view(text.size(0), -1, 1, 1)\n",
    "        c_code = text.repeat(1, 1, 4, 4)\n",
    "        \n",
    "        c_code = torch.cat((cond, c_code), 1)\n",
    "        \n",
    "        patch = self.conv_patch(c_code)\n",
    "        return patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    IMG_EXTENSIONS = ['.jpg', '.jpeg', '.png', '.bmp']\n",
    "    \n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.paths = self.get_paths(img_dir)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def get_paths(self, img_dir):\n",
    "        img_dir = Path(img_dir)\n",
    "        paths = [p for p in img_dir.iterdir() if p.suffix in ImageDataset.IMG_EXTENSIONS]\n",
    "        return paths\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        path = self.paths[index]\n",
    "        image = Image.open(path)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextData:\n",
    "    def __init__(self, csv_path):\n",
    "        df = pd.read_csv(csv_path, sep='|')\n",
    "\n",
    "        print('Read filenames.')\n",
    "        self.filenames = df.iloc[:,0].tolist()\n",
    "        print('Tokenize texts.')\n",
    "        self.texts = [TextData.tokenizer(str(text)) for text in tqdm(df.iloc[:,2].tolist())]\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenizer(text):\n",
    "        for s in string.punctuation:\n",
    "            if not(s == '.' or s == ','):\n",
    "                text = text.replace(s, ' ')\n",
    "            else:\n",
    "                text = text.replace(s, ' ' + s + ' ')\n",
    "        text = text.strip().split()\n",
    "        return text\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.filenames[index], self.texts[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def tolist(self):\n",
    "        return sum(self.texts, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextAndImageDataset(ImageDataset):\n",
    "    @staticmethod\n",
    "    def make_vocab(text_data: TextData, vocab_size=None):\n",
    "        print('Generate word-ids.')\n",
    "        word2id = {}\n",
    "        word2id['<pad>'] = 0\n",
    "        word2id['<unk>'] = 1\n",
    "        \n",
    "        #wc = collections.Counter(text_data.tolist())\n",
    "        #for i, (w, _) in enumerate(wc.most_common(vocab_size), 2):\n",
    "        #    word2id[w] = i\n",
    "        \n",
    "        id2word = {v: k for k, v in word2id.items()}\n",
    "\n",
    "        for texts in tqdm(text_data):\n",
    "            for words in texts:\n",
    "                for word in words:\n",
    "                    if word not in word2id:\n",
    "                        id = len(word2id)\n",
    "                        word2id[word] = id\n",
    "                        id2word[id] = word\n",
    "        \n",
    "        return word2id, id2word\n",
    "    \n",
    "    def __init__(self, csv_path, img_dir, sentence_size, vocab_size=None, transform=None):\n",
    "        super().__init__(img_dir, transform)\n",
    "        \n",
    "        self.sentence_size = sentence_size\n",
    "        \n",
    "        if os.path.exists('textdata.dat'):\n",
    "            with open(os.path.join('.', 'textdata.dat'), 'rb') as f:\n",
    "                self.text_data = load(f)\n",
    "                print('Loaded textdata.dat.')\n",
    "        else:\n",
    "            self.text_data = TextData(csv_path)\n",
    "            with open(os.path.join('.', 'textdata.dat'), 'wb') as f:\n",
    "                dump(self.text_data, f)\n",
    "                print('Saved textdata.dat.')\n",
    "        \n",
    "        if os.path.exists('word2id.dat') and os.path.exists('id2word.dat'):\n",
    "            with open(os.path.join('.', 'word2id.dat'), 'rb') as f:\n",
    "                self.word2id = load(f)\n",
    "                print('Loaded word2id.dat.')\n",
    "            with open(os.path.join('.', 'id2word.dat'), 'rb') as f:\n",
    "                self.id2word = load(f)\n",
    "                print('Loaded id2word.dat.')\n",
    "        else:\n",
    "            self.word2id, self.id2word = TextAndImageDataset.make_vocab(self.text_data, vocab_size)\n",
    "            with open(os.path.join('.', 'word2id.dat'), 'wb') as f:\n",
    "                dump(self.word2id, f)\n",
    "                print('Saved word2id.dat.')\n",
    "            with open(os.path.join('.', 'id2word.dat'), 'wb') as f:\n",
    "                dump(self.id2word, f)\n",
    "                print('Saved id2word.dat.')\n",
    "        \n",
    "    \n",
    "    def to_string(self, tensor):\n",
    "        text = ''\n",
    "        for d in tensor.tolist():\n",
    "            text += self.id2word[d] + ' '\n",
    "        return text\n",
    "    \n",
    "    def to_tokens(self, data):\n",
    "        tokens = []\n",
    "        for d in data:\n",
    "            tokens += [self.word2id[d] if d in self.word2id else self.word2id['<unk>']]\n",
    "        return tokens\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        filename, text = self.text_data[index]\n",
    "        \n",
    "        path = [path for path in self.paths if filename == path.name][0]\n",
    "        image = Image.open(path)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            toTensor = transforms.ToTensor()\n",
    "            image = toTensor(image)\n",
    "        \n",
    "        text = self.to_tokens(text)\n",
    "        text.extend([self.word2id['<pad>'] for _ in range(self.sentence_size - len(text))])\n",
    "        text = torch.LongTensor(text)\n",
    "        \n",
    "        return image, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "meoyoQHI7tAm"
   },
   "outputs": [],
   "source": [
    "class Util:\n",
    "    @staticmethod\n",
    "    def loadImages(batch_size, folder_path, size):\n",
    "        imgs = ImageFolder(folder_path, transform=transforms.Compose([\n",
    "            transforms.Resize(int(size)),\n",
    "            transforms.RandomCrop(size),\n",
    "            transforms.ToTensor()\n",
    "        ]))\n",
    "        return DataLoader(imgs, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver:\n",
    "    def __init__(self, args):\n",
    "        has_cuda = torch.cuda.is_available() if not args.cpu else False\n",
    "        self.device = torch.device(\"cuda\" if has_cuda else \"cpu\")\n",
    "        \n",
    "        self.args = args\n",
    "        self.pseudo_aug = 0.0\n",
    "        self.epoch = 0\n",
    "        \n",
    "        self.load_dataset()\n",
    "        \n",
    "        self.CLIP = CLIP(self.args.CLIP_max_patches, self.args.CLIP_patch_size,\n",
    "                         len(self.dataset.word2id), self.args.CLIP_sentence_size).to(self.device)\n",
    "        self.CLIP.apply(self.weights_init)\n",
    "        \n",
    "        self.stage1_g = STAGE1_G().to(self.device)\n",
    "        self.stage1_d = STAGE1_D().to(self.device)\n",
    "        self.stage2_g = STAGE2_G().to(self.device)\n",
    "        self.stage2_d = STAGE2_D().to(self.device)\n",
    "\n",
    "        self.stage1_g.apply(self.weights_init)\n",
    "        self.stage1_d.apply(self.weights_init)\n",
    "        self.stage2_g.apply(self.weights_init)\n",
    "        self.stage2_d.apply(self.weights_init)\n",
    "        \n",
    "        self.optimizer_CLIP = optim.Adam(self.CLIP.parameters(), lr=self.args.lr, betas=(0, 0.9))\n",
    "        self.optimizer_G = optim.Adam(itertools.chain(self.stage1_g.parameters(),\n",
    "                                                      self.stage2_g.parameters()),\n",
    "                                      lr=self.args.lr, betas=(0, 0.9))\n",
    "        self.optimizer_D = optim.Adam(itertools.chain(self.stage1_d.parameters(),\n",
    "                                                      self.stage2_d.parameters()),\n",
    "                                      lr=self.args.lr * self.args.mul_lr_dis, betas=(0, 0.9))\n",
    "    \n",
    "    def weights_init(self, module):\n",
    "        if type(module) == nn.Linear or type(module) == nn.Conv2d or type(module) == nn.ConvTranspose2d:\n",
    "            nn.init.kaiming_normal_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.fill_(0)\n",
    "            \n",
    "    def load_dataset(self, img_size=256):\n",
    "        self.dataset = TextAndImageDataset(self.args.csv_path, self.args.image_dir, self.args.CLIP_sentence_size,\n",
    "                                           transform=transforms.Compose([\n",
    "                                               transforms.Resize(int(img_size)),\n",
    "                                               transforms.RandomCrop(img_size),\n",
    "                                               transforms.RandomHorizontalFlip(),\n",
    "                                               transforms.ToTensor()\n",
    "                                           ]))\n",
    "        self.dataloader = DataLoader(self.dataset, batch_size=self.args.batch_size,\n",
    "                                     shuffle=True, drop_last=True, num_workers=os.cpu_count())\n",
    "        self.max_iters = len(iter(self.dataloader))\n",
    "            \n",
    "    def save_state(self, epoch):\n",
    "        self.CLIP.cpu(), self.stage1_g.cpu(), self.stage1_d.cpu(), self.stage2_g.cpu(), self.stage2_d.cpu()\n",
    "        torch.save(self.CLIP.state_dict(), os.path.join(self.args.weight_dir, f'weight_CLIP.{epoch}.pth'))\n",
    "        torch.save(self.stage1_g.state_dict(), os.path.join(self.args.weight_dir, f'weight_G1.{epoch}.pth'))\n",
    "        torch.save(self.stage1_d.state_dict(), os.path.join(self.args.weight_dir, f'weight_D1.{epoch}.pth'))\n",
    "        torch.save(self.stage2_g.state_dict(), os.path.join(self.args.weight_dir, f'weight_G2.{epoch}.pth'))\n",
    "        torch.save(self.stage2_d.state_dict(), os.path.join(self.args.weight_dir, f'weight_D2.{epoch}.pth'))\n",
    "        self.CLIP.to(self.device), self.stage1_g.to(self.device), self.stage1_d.to(self.device), self.stage2_g.to(self.device), self.stage2_d.to(self.device)\n",
    "        \n",
    "    def load_state(self):\n",
    "        if os.path.exists('weight_CLIP.pth'):\n",
    "            self.CLIP.load_state_dict(torch.load('weight_CLIP.pth', map_location=self.device))\n",
    "            print('Loaded CLIP network state.')\n",
    "        if os.path.exists('weight_G1.pth'):\n",
    "            self.stage1_g.load_state_dict(torch.load('weight_G1.pth', map_location=self.device))\n",
    "            print('Loaded Stage1_G network state.')\n",
    "        if os.path.exists('weight_D1.pth'):\n",
    "            self.stage1_d.load_state_dict(torch.load('weight_D1.pth', map_location=self.device))\n",
    "            print('Loaded Stage1_D network state.')\n",
    "        if os.path.exists('weight_G2.pth'):\n",
    "            self.stage2_g.load_state_dict(torch.load('weight_G2.pth', map_location=self.device))\n",
    "            print('Loaded Stage2_G network state.')\n",
    "        if os.path.exists('weight_D2.pth'):\n",
    "            self.stage2_d.load_state_dict(torch.load('weight_D2.pth', map_location=self.device))\n",
    "            print('Loaded Stage2_D network state.')\n",
    "    \n",
    "    def save_resume(self):\n",
    "        with open(os.path.join('.', f'resume.pkl'), 'wb') as f:\n",
    "            dump(self, f)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load(args, resume=True):\n",
    "        if resume and os.path.exists('resume.pkl'):\n",
    "            with open(os.path.join('.', 'resume.pkl'), 'rb') as f:\n",
    "                print('Load resume.')\n",
    "                solver = load(f)\n",
    "                solver.args = args\n",
    "                return solver\n",
    "        else:\n",
    "            return Solver(args)\n",
    "    \n",
    "    def trainCLIP(self, images, texts):\n",
    "        clip_loss = self.CLIP(images, texts)\n",
    "        \n",
    "        # Backward and optimize.\n",
    "        self.optimizer_CLIP.zero_grad()\n",
    "        clip_loss.backward()\n",
    "        self.optimizer_CLIP.step()\n",
    "        \n",
    "        # Logging.\n",
    "        loss = {}\n",
    "        loss['CLIP/loss'] = clip_loss.item()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def trainGAN(self, epoch, iters, max_iters, real_img, texts, a=0, b=1, c=1):\n",
    "        ### Train with LSGAN.\n",
    "        ### for example, (a, b, c) = 0, 1, 1 or (a, b, c) = -1, 1, 0\n",
    "        \n",
    "        resize_64 = transforms.Resize(64)\n",
    "        resize_256 = transforms.Resize(256)\n",
    "        real_img_64 = resize_64(real_img)\n",
    "        real_img_256 = resize_256(real_img)\n",
    "        loss = {}\n",
    "        \n",
    "        # ================================================================================ #\n",
    "        #                             Train the CLIP                                       #\n",
    "        # ================================================================================ #\n",
    "        \n",
    "        self.CLIP.train()\n",
    "        loss = self.trainCLIP(real_img_256, texts)\n",
    "        self.CLIP.eval()\n",
    "        \n",
    "        # ================================================================================ #\n",
    "        #                             Train the discriminator                              #\n",
    "        # ================================================================================ #\n",
    "        \n",
    "        text = self.CLIP.text_encoder(texts)\n",
    "        text = text.detach()\n",
    "        \n",
    "        fake_img_1, vc_loss_1, noise = self.stage1_g(text)\n",
    "        fake_score_1 = self.stage1_d(fake_img_1, text)\n",
    "        \n",
    "        fake_img_2, vc_loss_2 = self.stage2_g(fake_img_1, text)\n",
    "        fake_score_2 = self.stage2_d(fake_img_2, text)\n",
    "        \n",
    "        # for Mode-Seeking\n",
    "        _fake_img_2 = Variable(fake_img_2.data)\n",
    "        _noise = Variable(noise.data)\n",
    "        \n",
    "        real_score_1 = self.stage1_d(real_img_64, text)\n",
    "        real_score_2 = self.stage2_d(real_img_256, text)\n",
    "        \n",
    "        # Compute loss with real images.\n",
    "        real_src_loss = torch.sum((real_score_1 + real_score_2 - b) ** 2)\n",
    "        \n",
    "        # Compute loss with fake images.\n",
    "        p = random.uniform(0, 1)\n",
    "        if 1 - self.pseudo_aug < p:\n",
    "            fake_src_loss = torch.sum((fake_score_1 + fake_score_2 - b) ** 2) # Pseudo: fake is real.\n",
    "        else:\n",
    "            fake_src_loss = torch.sum((fake_score_1 + fake_score_2 - a) ** 2)\n",
    "        \n",
    "        vc_loss = (vc_loss_1 + vc_loss_2) * 1e-5\n",
    "        \n",
    "        # Update Probability Augmentation.\n",
    "        lz = (torch.sign(torch.logit(real_score_1 + real_score_2)).mean()\n",
    "              - torch.sign(torch.logit(fake_score_1 + fake_score_2)).mean()) / 2\n",
    "        if lz > self.args.aug_threshold:\n",
    "            self.pseudo_aug += 0.01\n",
    "        else:\n",
    "            self.pseudo_aug -= 0.01\n",
    "        self.pseudo_aug = min(1, max(0, self.pseudo_aug))\n",
    "        \n",
    "        # Backward and optimize.\n",
    "        d_loss = 0.5 * (real_src_loss + fake_src_loss) / self.args.batch_size + vc_loss\n",
    "        self.optimizer_D.zero_grad()\n",
    "        d_loss.backward()\n",
    "        self.optimizer_D.step()\n",
    "        \n",
    "        # Logging.\n",
    "        loss['D/loss'] = d_loss.item()\n",
    "        loss['D/vc_loss'] = vc_loss.item()\n",
    "        loss['D/pseudo_aug'] = self.pseudo_aug\n",
    "        \n",
    "        # ================================================================================ #\n",
    "        #                               Train the generator                                #\n",
    "        # ================================================================================ #\n",
    "        \n",
    "        text = self.CLIP.text_encoder(texts)\n",
    "        text = text.detach()\n",
    "        \n",
    "        fake_img_1, vc_loss_1, noise = self.stage1_g(text)\n",
    "        fake_score_1 = self.stage1_d(fake_img_1, text)\n",
    "        \n",
    "        fake_img_2, vc_loss_2 = self.stage2_g(fake_img_1, text)\n",
    "        fake_score_2 = self.stage2_d(fake_img_2, text)\n",
    "        \n",
    "        # Compute loss with fake images.\n",
    "        fake_src_loss = torch.sum((fake_score_1 + fake_score_2 - c) ** 2)\n",
    "        \n",
    "        # Mode Seeking Loss\n",
    "        lz = torch.mean(torch.abs(fake_img_2 - _fake_img_2)) / torch.mean(torch.abs(noise - _noise))\n",
    "        eps = 1 * 1e-5\n",
    "        ms_loss = 1 / (lz + eps)\n",
    "        \n",
    "        # Backward and optimize.\n",
    "        g_loss = 0.5 * fake_src_loss / self.args.batch_size + self.args.lambda_ms * ms_loss\n",
    "        self.optimizer_G.zero_grad()\n",
    "        g_loss.backward()\n",
    "        self.optimizer_G.step()\n",
    "\n",
    "        # Logging.\n",
    "        loss['G/loss'] = g_loss.item()\n",
    "        loss['G/ms_loss'] = ms_loss.item()\n",
    "        \n",
    "        # Save\n",
    "        if iters == max_iters:\n",
    "            self.save_state('last')\n",
    "            img_name = str(epoch) + '_' + str(iters) + '_1.png'\n",
    "            img_path = os.path.join(self.args.result_dir, img_name)\n",
    "            save_image(fake_img_1, img_path)\n",
    "            img_name = str(epoch) + '_' + str(iters) + '_2.png'\n",
    "            img_path = os.path.join(self.args.result_dir, img_name)\n",
    "            save_image(fake_img_2, img_path)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def train(self):\n",
    "        print(f'Use Device: {self.device}')\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        \n",
    "        self.CLIP.eval()\n",
    "        self.stage1_g.train()\n",
    "        self.stage1_d.train()\n",
    "        self.stage2_g.train()\n",
    "        self.stage2_d.train()\n",
    "        \n",
    "        hyper_params = {}\n",
    "        hyper_params['CSV Path'] = self.args.csv_path\n",
    "        hyper_params['Image Dir'] = self.args.image_dir\n",
    "        hyper_params['Result Dir'] = self.args.result_dir\n",
    "        hyper_params['Weight Dir'] = self.args.weight_dir\n",
    "        hyper_params['CLIP_max_patches'] = self.args.CLIP_max_patches\n",
    "        hyper_params['CLIP_patch_size'] = self.args.CLIP_patch_size\n",
    "        hyper_params['CLIP_sentence_size'] = self.args.CLIP_sentence_size\n",
    "        hyper_params['Prob-Aug-Threshold'] = self.args.aug_threshold\n",
    "        hyper_params['Learning Rate'] = self.args.lr\n",
    "        hyper_params[\"Mul Discriminator's LR\"] = self.args.mul_lr_dis\n",
    "        hyper_params['Batch Size'] = self.args.batch_size\n",
    "        hyper_params['Num Train'] = self.args.num_train\n",
    "        hyper_params['Lambda Mode-Seeking'] = self.args.lambda_ms\n",
    "\n",
    "        for key in hyper_params.keys():\n",
    "            print(f'{key}: {hyper_params[key]}')\n",
    "        #experiment.log_parameters(hyper_params)\n",
    "        \n",
    "        while self.args.num_train > self.epoch:\n",
    "            self.epoch += 1\n",
    "            epoch_loss_CLIP = 0.0\n",
    "            epoch_loss_G = 0.0\n",
    "            epoch_loss_D = 0.0\n",
    "            \n",
    "            for iters, (images, texts) in enumerate(tqdm(self.dataloader)):\n",
    "                iters += 1\n",
    "                \n",
    "                images = images.to(self.device, non_blocking=True)\n",
    "                texts = texts.to(self.device, non_blocking=True)\n",
    "                \n",
    "                loss = self.trainGAN(self.epoch, iters, self.max_iters, images, texts)\n",
    "                \n",
    "                epoch_loss_CLIP += loss['CLIP/loss']\n",
    "                epoch_loss_G += loss['G/loss']\n",
    "                epoch_loss_D += loss['D/loss']\n",
    "                #experiment.log_metrics(loss)\n",
    "            \n",
    "            epoch_loss = epoch_loss_CLIP + epoch_loss_G + epoch_loss_D\n",
    "            \n",
    "            print(f'Epoch[{self.epoch}] CLIP({epoch_loss_CLIP}) + G({epoch_loss_G}) + D({epoch_loss_D}) = {epoch_loss}')\n",
    "                    \n",
    "            if not self.args.noresume:\n",
    "                self.save_resume()\n",
    "    \n",
    "    def generate(self, text):\n",
    "        self.CLIP.eval()\n",
    "        self.stage1_g.eval()\n",
    "        self.stage2_g.eval()\n",
    "        \n",
    "        text = self.dataset.to_tokens(TextData.tokenizer(text))\n",
    "        text.extend([self.dataset.word2id['<pad>'] for _ in range(self.dataset.sentence_size - len(text))])\n",
    "        text = torch.LongTensor(text).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        print(self.dataset.to_string(text[0]))\n",
    "        \n",
    "        text = self.CLIP.text_encoder(text)\n",
    "        fake_img_1, _, _ = self.stage1_g(text)\n",
    "        fake_img_2, _ = self.stage2_g(fake_img_1, text)\n",
    "\n",
    "        save_image(fake_img_1[0], os.path.join(self.args.result_dir, f'generated_1_{time.time()}.png'))\n",
    "        save_image(fake_img_2[0], os.path.join(self.args.result_dir, f'generated_2_{time.time()}.png'))\n",
    "        print('New picture was generated.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    solver = Solver.load(args, resume=not args.noresume)\n",
    "    solver.load_state()\n",
    "    \n",
    "    if args.generate != '':\n",
    "        solver.generate(args.generate)\n",
    "        return\n",
    "    \n",
    "    solver.train()\n",
    "    #experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--csv_path', type=str, default='/mnt/c/Datasets/flickr-images-dataset/results.csv')\n",
    "    parser.add_argument('--image_dir', type=str, default='/mnt/c/Datasets/flickr-images-dataset/flickr30k_images/')\n",
    "    parser.add_argument('--result_dir', type=str, default='results')\n",
    "    parser.add_argument('--weight_dir', type=str, default='weights')\n",
    "    parser.add_argument('--CLIP_max_patches', type=int, default=128)\n",
    "    parser.add_argument('--CLIP_patch_size', type=int, default=32)\n",
    "    parser.add_argument('--CLIP_sentence_size', type=int, default=128)\n",
    "    parser.add_argument('--aug_threshold', type=float, default=0.6)\n",
    "    parser.add_argument('--lr', type=float, default=0.0001)\n",
    "    parser.add_argument('--mul_lr_dis', type=float, default=4)\n",
    "    parser.add_argument('--batch_size', type=int, default=8)\n",
    "    parser.add_argument('--num_train', type=int, default=100)\n",
    "    parser.add_argument('--lambda_ms', type=float, default=1)\n",
    "    parser.add_argument('--cpu', action='store_true')\n",
    "    parser.add_argument('--noresume', action='store_true')\n",
    "    parser.add_argument('--generate', type=str, default='')\n",
    "    \n",
    "    args, unknown = parser.parse_known_args()\n",
    "    \n",
    "    if not os.path.exists(args.result_dir):\n",
    "        os.mkdir(args.result_dir)\n",
    "    if not os.path.exists(args.weight_dir):\n",
    "        os.mkdir(args.weight_dir)\n",
    "    \n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CycleGAN.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
